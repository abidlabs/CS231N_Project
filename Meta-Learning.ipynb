{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "\n",
    "import numpy as np, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "params = {\n",
    "   'axes.labelsize': 16,\n",
    "   'font.size': 16,\n",
    "   'legend.fontsize': 16,\n",
    "   'xtick.labelsize': 13,\n",
    "   'ytick.labelsize': 13,\n",
    "   'text.usetex': False,\n",
    "    'font.family':\"sans-serif\",\n",
    "   'font.sans-serif':'Arial',\n",
    "   'text.usetex': False,\n",
    "   }\n",
    "\n",
    "#mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "mnist = input_data.read_data_sets(\"data/fashion\", one_hot=True)\n",
    "\n",
    "rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = 4\n",
    "d = int(784/pool**2)\n",
    "lr = 0.1\n",
    "lr2 = 500\n",
    "batches = 8000\n",
    "print_every = 1000\n",
    "num_channels=10\n",
    "verbose=True\n",
    "\n",
    "def HZ_reg(weights):\n",
    "    d1, d2 = weights.shape\n",
    "    d1, d2 = int(d1), int(d2)\n",
    "    n_chan = d2//d1\n",
    "    weights_rolled = tf.manip.roll(tf.reshape(weights,[d1,d1,n_chan]), shift=[-1], axis=[0])\n",
    "    result = tf.reduce_mean(tf.abs(tf.reshape(weights,[d1,d1,n_chan])-weights_rolled))\n",
    "    return result\n",
    "\n",
    "def DZ_reg(weights):\n",
    "    d1, d2 = weights.shape\n",
    "    d1, d2 = int(d1), int(d2)\n",
    "    n_chan = d2//d1\n",
    "    weights_rolled = tf.manip.roll(tf.reshape(weights,[d1,d1,n_chan]), shift=[-1,-1], axis=[0,1])\n",
    "    result = tf.reduce_mean(tf.abs(tf.reshape(weights,[d1,d1,n_chan])-weights_rolled))\n",
    "    return result\n",
    "\n",
    "def shift_reg(weights, shift):\n",
    "    d1, d2 = weights.shape\n",
    "    d1, d2 = int(d1), int(d2)\n",
    "    n_chan = d2//d1\n",
    "    weights_rolled = tf.manip.roll(tf.reshape(weights,[d1,d1,n_chan]), shift=shift, axis=[0,1])\n",
    "    result = tf.reduce_mean(tf.abs(tf.reshape(weights,[d1,d1,n_chan])-weights_rolled))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x_train = tf.placeholder(tf.float32, [None, 784])\n",
    "y_train = tf.placeholder(tf.float32, [None, 10])\n",
    "x_valid = tf.placeholder(tf.float32, [None, 784])\n",
    "y_valid = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "alpha = get_variable([], initial=0.0)\n",
    "beta = get_variable([], initial=0.0)\n",
    "\n",
    "#hidden layer\n",
    "W_fc1 = get_variable([d, d*num_channels])\n",
    "b_fc1 = get_variable([d*num_channels])\n",
    "W_fc2 = get_variable([d*num_channels, 10])\n",
    "b_fc2 = get_variable([10])\n",
    "\n",
    "def network(x):\n",
    "    x_reshaped = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    x_reshaped = tf.nn.max_pool(x_reshaped, [1, pool, pool,1], [1, pool, pool, 1], padding='SAME')\n",
    "    x_reshaped = tf.reshape(x_reshaped, [-1, d])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_reshaped, W_fc1) + b_fc1)\n",
    "    y_train_ = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    return y_train_\n",
    "\n",
    "y_train_ = network(x_train)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_train, logits=y_train_))\n",
    "loss = loss + alpha * DZ_reg(W_fc1)\n",
    "loss = loss + beta  * HZ_reg(W_fc1)\n",
    "\n",
    "dW1 = tf.gradients(loss, W_fc1)[0]\n",
    "db1 = tf.gradients(loss, b_fc1)[0]\n",
    "dW2 = tf.gradients(loss, W_fc2)[0]\n",
    "db2 = tf.gradients(loss, b_fc2)[0]\n",
    "\n",
    "with tf.get_default_graph().control_dependencies([dW1, db1, dW2, db2]):\n",
    "    updateW1 = tf.assign_sub(W_fc1, lr*dW1)\n",
    "    updateb1 = tf.assign_sub(b_fc1, lr*db1)\n",
    "    updateW2 = tf.assign_sub(W_fc2, lr*dW2)\n",
    "    updateb2 = tf.assign_sub(b_fc2, lr*db2)\n",
    "grp = tf.group(dW1, db1, dW2, db2, updateW1, updateb1, updateW2, updateb2)\n",
    "    \n",
    "def meta_network(x):\n",
    "    x_reshaped = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    x_reshaped = tf.nn.max_pool(x_reshaped, [1, pool, pool,1], [1, pool, pool, 1], padding='SAME')\n",
    "    x_reshaped = tf.reshape(x_reshaped, [-1, d])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_reshaped, W_fc1 - lr*dW1) + b_fc1 - lr*db1)\n",
    "    y_train_ = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    return y_train_\n",
    "\n",
    "y_valid_ = meta_network(x_valid)\n",
    "meta_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_valid, logits=y_valid_))\n",
    "\n",
    "dAlpha = tf.gradients(meta_loss, alpha)[0]\n",
    "updateAlpha = tf.assign_sub(alpha, lr2*dAlpha)\n",
    "dBeta = tf.gradients(meta_loss, beta)[0]\n",
    "updateBeta = tf.assign_sub(beta, lr2*dBeta)\n",
    "\n",
    "grp2 = tf.group(dAlpha, updateAlpha, dBeta, updateBeta)\n",
    "\n",
    "losses = list()\n",
    "alphas = list()\n",
    "betas = list()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_train, 1), tf.argmax(y_train_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()        \n",
    "    for i in range(batches):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        batch_valid = mnist.test.next_batch(100)\n",
    "        \n",
    "        sess.run(grp, feed_dict={x_train: batch[0], y_train: batch[1]})\n",
    "        l = sess.run(loss, feed_dict={x_train: batch[0], y_train: batch[1]})\n",
    "        sess.run(grp2, feed_dict={x_train: batch[0], y_train: batch[1],\n",
    "                                      x_valid: batch_valid[0], \n",
    "                                      y_valid: batch_valid[1]})\n",
    "        \n",
    "        al = alpha.eval()        \n",
    "        be = beta.eval()        \n",
    "        weights = W_fc1.eval()\n",
    "        \n",
    "        if i%print_every==0:\n",
    "            losses.append(l)\n",
    "            alphas.append(al)\n",
    "            betas.append(be)\n",
    "            if verbose:\n",
    "                print(i)\n",
    "                print(\"Loss:\",l)\n",
    "                print('alpha:',al)\n",
    "                print('beta:',be)\n",
    "                \n",
    "    accuracy_train = accuracy.eval(feed_dict={x_train: mnist.train.images[:1000], y_train: mnist.train.labels[:1000]})\n",
    "    accuracy_valid = accuracy.eval(feed_dict={x_train: mnist.test.images, y_train: mnist.test.labels})\n",
    "\n",
    "print(i)\n",
    "print(\"Loss:\",l)\n",
    "print('alpha:',al)\n",
    "print('beta:',be)  \n",
    "\n",
    "print(\"Training accuracy:\",accuracy_train)\n",
    "print(\"Validation accuracy:\",accuracy_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,6])\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(alphas, label='Diagonal')\n",
    "plt.plot(betas, label='Horizontal')\n",
    "plt.ylabel('Hyperparameter')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(losses, label='Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-44ce0133227e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize_general_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "visualize_general_weights(weights, num_channels=num_channels, pool=pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Learning with Many Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x_train = tf.placeholder(tf.float32, [None, 784])\n",
    "y_train = tf.placeholder(tf.float32, [None, 10])\n",
    "x_valid = tf.placeholder(tf.float32, [None, 784])\n",
    "y_valid = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "alpha = get_variable([], initial=0.0)\n",
    "beta = get_variable([], initial=0.0)\n",
    "\n",
    "#hidden layer\n",
    "W_fc1 = get_variable([d, d*num_channels])\n",
    "b_fc1 = get_variable([d*num_channels])\n",
    "W_fc2 = get_variable([d*num_channels, 10])\n",
    "b_fc2 = get_variable([10])\n",
    "\n",
    "def network(x):\n",
    "    x_reshaped = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    x_reshaped = tf.nn.max_pool(x_reshaped, [1, pool, pool,1], [1, pool, pool, 1], padding='SAME')\n",
    "    x_reshaped = tf.reshape(x_reshaped, [-1, d])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_reshaped, W_fc1) + b_fc1)\n",
    "    y_train_ = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    return y_train_\n",
    "\n",
    "y_train_ = network(x_train)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_train, logits=y_train_))\n",
    "loss = loss + alpha * DZ_reg(W_fc1)\n",
    "loss = loss + beta  * HZ_reg(W_fc1)\n",
    "\n",
    "dW1 = tf.gradients(loss, W_fc1)[0]\n",
    "db1 = tf.gradients(loss, b_fc1)[0]\n",
    "dW2 = tf.gradients(loss, W_fc2)[0]\n",
    "db2 = tf.gradients(loss, b_fc2)[0]\n",
    "\n",
    "with tf.get_default_graph().control_dependencies([dW1, db1, dW2, db2]):\n",
    "    updateW1 = tf.assign_sub(W_fc1, lr*dW1)\n",
    "    updateb1 = tf.assign_sub(b_fc1, lr*db1)\n",
    "    updateW2 = tf.assign_sub(W_fc2, lr*dW2)\n",
    "    updateb2 = tf.assign_sub(b_fc2, lr*db2)\n",
    "grp = tf.group(dW1, db1, dW2, db2, updateW1, updateb1, updateW2, updateb2)\n",
    "    \n",
    "def meta_network(x):\n",
    "    x_reshaped = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    x_reshaped = tf.nn.max_pool(x_reshaped, [1, pool, pool,1], [1, pool, pool, 1], padding='SAME')\n",
    "    x_reshaped = tf.reshape(x_reshaped, [-1, d])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_reshaped, W_fc1 - lr*dW1) + b_fc1 - lr*db1)\n",
    "    y_train_ = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    return y_train_\n",
    "\n",
    "y_valid_ = meta_network(x_valid)\n",
    "meta_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_valid, logits=y_valid_))\n",
    "\n",
    "dAlpha = tf.gradients(meta_loss, alpha)[0]\n",
    "updateAlpha = tf.assign_sub(alpha, lr2*dAlpha)\n",
    "dBeta = tf.gradients(meta_loss, beta)[0]\n",
    "updateBeta = tf.assign_sub(beta, lr2*dBeta)\n",
    "\n",
    "grp2 = tf.group(dAlpha, updateAlpha, dBeta, updateBeta)\n",
    "\n",
    "losses = list()\n",
    "alphas = list()\n",
    "betas = list()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_train, 1), tf.argmax(y_train_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()        \n",
    "    for i in range(batches):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        batch_valid = mnist.test.next_batch(100)\n",
    "        \n",
    "        sess.run(grp, feed_dict={x_train: batch[0], y_train: batch[1]})\n",
    "        l = sess.run(loss, feed_dict={x_train: batch[0], y_train: batch[1]})\n",
    "        sess.run(grp2, feed_dict={x_train: batch[0], y_train: batch[1],\n",
    "                                      x_valid: batch_valid[0], \n",
    "                                      y_valid: batch_valid[1]})\n",
    "        \n",
    "        al = alpha.eval()        \n",
    "        be = beta.eval()        \n",
    "        weights = W_fc1.eval()\n",
    "        \n",
    "        if i%print_every==0:\n",
    "            losses.append(l)\n",
    "            alphas.append(al)\n",
    "            betas.append(be)\n",
    "            if verbose:\n",
    "                print(i)\n",
    "                print(\"Loss:\",l)\n",
    "                print('alpha:',al)\n",
    "                print('beta:',be)\n",
    "                \n",
    "    accuracy_train = accuracy.eval(feed_dict={x_train: mnist.train.images[:1000], y_train: mnist.train.labels[:1000]})\n",
    "    accuracy_valid = accuracy.eval(feed_dict={x_train: mnist.test.images, y_train: mnist.test.labels})\n",
    "\n",
    "print(i)\n",
    "print(\"Loss:\",l)\n",
    "print('alpha:',al)\n",
    "print('beta:',be)  \n",
    "\n",
    "print(\"Training accuracy:\",accuracy_train)\n",
    "print(\"Validation accuracy:\",accuracy_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
